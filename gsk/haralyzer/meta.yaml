{% set name = "haralyzer" %}
{% set version = "1.4.10" %}
{% set hash_type = "sha256" %}
{% set hash_value = "ef22c579366aa10c5675c37bf351f54443415914b2326350f1a78f9e707bf213" %}

package:
  version: '{{ version }}'
  name: '{{ name|lower }}'

source:
  url: https://pypi.io/packages/source/{{ name[0] }}/{{ name }}/{{ name }}-{{ version }}.tar.gz
  '{{ hash_type }}': '{{ hash_value }}'
  fn: '{{ name }}-{{ version }}.tar.gz'

build:
  number: 0
  script: python setup.py install  --single-version-externally-managed --record=record.txt

requirements:
  run:
    - python
    - cached-property
    - python-dateutil
    - backports.statistics
  build:
    - python
    - setuptools
    - cached-property
    - python-dateutil
    - backports.statistics

test:
  imports:
    - haralyzer
  requires:
    - pytest-cov
    - python-coveralls

about:
  dev_url: ''
  description: "=========\nHaralyzer\n=========\n\n.. image:: https://badge.fury.io/py/haralyzer.svg\n    :target: http://badge.fury.io/py/haralyzer\n\n.. image:: https://travis-ci.org/mrname/haralyzer.svg?branch=master\n\
    \    :target: https://travis-ci.org/mrname/haralyzer\n\n.. image:: https://coveralls.io/repos/mrname/haralyzer/badge.svg?branch=master\n  :target: https://coveralls.io/r/mrname/haralyzer?branch=master\n\
    \n.. image:: https://readthedocs.org/projects/haralyzer/badge/?version=latest\n    :target: http://haralyzer.readthedocs.org/en/latest/\n\nA Python Framework For Using HAR Files To Analyze Web Pages.\n\
    \nOverview\n--------\n\nThe haralyzer module contains two classes for analyzing web pages based\non a HAR file. ``HarParser()`` represents a full file (which might have\nmultiple pages), and ``HarPage()``\
    \ represents a single page from said file.\n\n``HarParser`` has a couple of helpful methods for analyzing single entries\nfrom a HAR file, but most of the pertinent functions are inside of the page\n\
    object.\n\n``haralyzer`` was designed to be easy to use, but you can also access more\npowerful functions directly.\n\nQuick Intro\n-----------\n\nHarParser\n+++++++++\n\nThe ``HarParser`` takes a single\
    \ argument of a ``dict`` representing the JSON\nof a full HAR file. It has the same properties of the HAR file, EXCEPT that each\npage in HarParser.pages is a HarPage object::\n\n    import json\n \
    \   from haralyzer import HarParser, HarPage\n\n    with open('har_data.har', 'r') as f:\n        har_parser = HarParser(json.loads(f.read()))\n\n    print har_parser.browser\n    # {u'name': u'Firefox',\
    \ u'version': u'25.0.1'}\n\n    print har_parser.hostname\n    # 'humanssuck.net'\n\n    for page in har_parser.pages:\n        assert isinstance(page, HarPage, None)\n        # returns True for each\n\
    \nHarPage\n+++++++\n\nThe ``HarPage`` object contains most of the goods you need to easily analyze a\npage. It has helper methods that are accessible, but most of the data you need is\nin properties\
    \ for easy access. You can create a HarPage object directly by giving\nit the page ID (yes, I know it is stupid, it's just how HAR is organized), and either\na ``HarParser`` with `har_parser=parser`,\
    \ or a ``dict`` representing the JSON of a full HAR\nfile (see example above) with `har_data=har_data`::\n\n    import json\n    From haralyzer import HarPage\n\n    with open('har_data.har', 'r') as\
    \ f:\n        har_page = HarPage('page_3', har_data=json.loads(f.read()))\n\n    ### GET BASIC INFO\n    har_page.hostname\n    # 'humanssuck.net'\n    har_page.url\n    $ 'http://humanssuck.net/about/'\n\
    \n    ### WORK WITH LOAD TIMES (all load times are in ms) ###\n\n    # Get image load time in milliseconds as rendered by the browser\n    har_page.image_load_time\n    # prints 713\n\n    # We could\
    \ do this with 'css', 'js', 'html', 'audio', or 'video'\n\n    ### WORK WITH SIZES (all sizes are in bytes) ###\n\n    # Get the total page size (with all assets)\n    har_page.page_size\n    # prints\
    \ 2423765\n\n    # Get the total image size\n    har_page.image_size\n    # prints 733488\n    # We could do this with 'css', 'js', 'html', 'audio', or 'video'\n\n\nMultiHarParser\n++++++++++++++\n\n\
    The ``MutliHarParser`` takes a ``list`` of ``dict``, each of which represents the JSON\nof a full HAR file. The concept here is that you can provide multiple HAR files of the\nsame page (representing\
    \ multiple test runs) and the ``MultiHarParser`` will provide\naggregate results for load times::\n\n    import json\n    from haralyzer import HarParser, HarPage\n\n    test_runs = []\n    with open('har_data1.har',\
    \ 'r') as f1:\n        test_runs.append( (json.loads( f1.read() ) )\n    with open('har_data2.har', 'r') as f2:\n        test_runs.append( (json.loads( f2.read() ) )\n\n    multi_har_parser = MultiHarParser(har_data=test_runs)\n\
    \n    # Get the mean for the time to first byte of all runs in MS\n    print multi_har_parser.time_to_first_byte\n    # 70\n\n    # Get the total page load time mean for all runs in MS\n    print multi_har_parser.load_time\n\
    \    # 150\n\n    # Get the javascript load time mean for all runs in MS\n    print multi_har_parser.js_load_time\n    # 50\n\n    # You can get the standard deviation for any of these as well\n   \
    \ # Let's get the standard deviation for javascript load time\n    print multi_har_parser.get_stdev('js')\n    # 5\n    # We can also do that with 'page' or 'ttfb' (time to first byte)\n    print multi_har_parser.get_stdev('page')\n\
    \    # 11\n    print multi_har_parser.get_stdev('ttfb')\n    # 10\n\n    ### DECIMAL PRECISION ###\n\n    # You will notice that all of the results are above. That is because\n    # the default decimal\
    \ precision for the multi parser is 0. However, you\n    # can pass whatever you want into the constructor to control this.\n\n    multi_har_parser = MultiHarParser(har_data=test_runs, decimal_precision=2)\n\
    \    print multi_har_parser.time_to_first_byte\n    # 70.15\n\n\nAdvanced Usage\n==============\n\n``HarPage`` includes a lot of helpful properties, but they are all\neasily produced using the public\
    \ methods of ``HarParser`` and ``HarPage``::\n\n    import json\n    from haralyzer import HarPage\n\n    with open('har_data.har', 'r') as f:\n        har_page = HarPage('page_3', har_data=json.loads(f.read()))\n\
    \n    ### ACCESSING FILES ###\n\n    # You can get a JSON representation of all assets using HarPage.entries #\n    for entry in har_page.entries:\n        if entry['startedDateTime'] == 'whatever I\
    \ expect':\n            ... do stuff ...\n\n    # It also has methods for filtering assets #\n    # Get a collection of entries that were images in the 2XX status code range #\n    entries = har_page.filter_entries(content_type='image.*',\
    \ status_code='2.*')\n    # This method can filter by:\n    # * content_type ('application/json' for example)\n    # * status_code ('200' for example)\n    # * request_type ('GET' for example)\n   \
    \ # * http_version ('HTTP/1.1' for example)\n    # It will use a regex by default, but you can also force a literal string match by passing regex=False\n\n    # Get the size of the collection we just\
    \ made #\n    collection_size = har_page.get_total_size(entries)\n\n    # We can also access files by type with a property #\n    for js_file in har_page.js_files:\n        ... do stuff ....\n\n   \
    \ ### GETTING LOAD TIMES ###\n\n    # Get the BROWSER load time for all images in the 2XX status code range #\n    load_time = har_page.get_load_time(content_type='image.*', status_code='2.*')\n\n \
    \   # Get the TOTAL load time for all images in the 2XX status code range #\n    load_time = har_page.get_load_time(content_type='image.*', status_code='2.*', async=False)\n\nThis could potentially\
    \ be out of date, so please check out the sphinx docs.\n\n\nMore.... Advanced Usage\n=======================\n\nAll of the HarPage methods above leverage stuff from the HarParser,\nsome of which can\
    \ be useful for more complex operations. They either\noperate on a single entry (from a HarPage) or a ``list`` of entries::\n\n    import json\n    from haralyzer import HarParser\n\n    with open('har_data.har',\
    \ 'r') as f:\n        har_parser = HarParser(json.loads(f.read()))\n\n    for page in har_parser.pages:\n        for entry in page.entries:\n            ### MATCH HEADERS ###\n            if har_parser.match_headers(entry,\
    \ 'Content-Type', 'image.*'):\n                print 'This would appear to be an image'\n            ### MATCH REQUEST TYPE ###\n            if har_parser.match_request_type(entry, 'GET'):\n       \
    \         print 'This is a GET request'\n            ### MATCH STATUS CODE ###\n            if har_parser.match_status_code(entry, '2.*'):\n                print 'Looks like all is well in the world'\n\
    \n\nAsset Timelines\n+++++++++++++++\n\nThe last helper function of ``HarParser`` requires it's own section, because it\nis odd, but can be helpful, especially for creating charts and reports.\n\nIt\
    \ can create an asset timeline, which gives you back a ``dict`` where each\nkey is a ``datetime`` object, and the value is a ``list`` of assets that were\nloading at that time. Each value of the ``list``\
    \ is a ``dict`` representing\nan entry from a page.\n\nIt takes a ``list`` of entries to analyze, so it assumes that you have\nalready filtered the entries you want to know about::\n\n    import json\n\
    \    from haralyzer import HarParser\n\n    with open('har_data.har', 'r') as f:\n        har_parser = HarParser(json.loads(f.read()))\n\n    ### CREATE A TIMELINE OF ALL THE ENTRIES ###\n    entries\
    \ = []\n    for page in har_parser.pages:\n        for entry in page.entries:\n            entries.append(entry)\n\n    timeline = har_parser.create_asset_timeline(entries)\n\n    for key, value in\
    \ timeline.items():\n        print(type(key))\n        # <type 'datetime.datetime'>\n        print(key)\n        # 2015-02-21 19:15:41.450000-08:00\n        print(type(value))\n        # <type 'list'>\n\
    \        print(value)\n        # Each entry in the list is an asset from the page\n        # [{u'serverIPAddress': u'157.166.249.67', u'cache': {}, u'startedDateTime': u'2015-02-21T19:15:40.351-08:00',\
    \ u'pageref': u'page_3', u'request': {u'cookies':............................\n\n\nWith this, you can examine the timeline for any number of assets. Since the key is a ``datetime``\nobject, this is\
    \ a heavy operation. We could always change this in the future, but for now,\nlimit the assets you give this method to only what you need to examine."
  license: MIT License
  license_family: MIT
  summary: A python framework for getting useful stuff out of HAR files
  home: https://github.com/mrname/haralyzer
  license_file: ''
  doc_url: ''

extra:
  recipe-maintainers:
    - brittainhard
